# This pipeline is designed for a one-time bulk import of your Windows Event Log CSV.

input {
  file {
    # This path is INSIDE the Docker container, which we set up in docker-compose.yml
    path => "/usr/share/logstash/data/10_1_1_15-windows-eventviewer_labeled.csv"
    start_position => "beginning"
    # This ensures Logstash re-reads the file every time it starts, which is good for testing.
    sincedb_path => "/dev/null"
  }
}

filter {
  # The CSV filter parses each line into the columns you identified.
  csv {
    separator => ","
    skip_header => true # We don't want to import the title row.
    # These are the exact column names from your CSV file.
    columns => ["Level","Date and Time","Source","Event ID","Task Category","Activity","Stage","DefenderResponse","Signature"]
  }

  # Convert the 'Event ID' from a string of text to a number for better searching.
  mutate {
    convert => {
      "Event ID" => "integer"
    }
  }

  # Rename fields to be compliant with the Elastic Common Schema (ECS) for better UI integration.
  mutate {
    rename => {
      "Event ID" => "[event][code]"
      "Activity" => "[message]"
      "Source" => "[log][source][name]"
      "Level" => "[log][level]"
    }
  }
}

output {
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    user => "elastic"
    password => "${ELASTIC_PASSWORD}"
    # We are sending this data to a new, dedicated index.
    index => "windows-event-logs"
  }
  # This will print logs to your console so you can see the progress.
  stdout { codec => rubydebug }
}
